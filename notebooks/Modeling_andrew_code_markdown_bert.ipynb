{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgvmxkHtHVtd",
    "outputId": "f1c06018-60de-444f-e310-d2c8f20b8b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.9.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.10.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Building jupyterlab assets (build:prod:minimize)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Kmx0M90i49_",
    "outputId": "06bf19d9-56af-4e0d-c7fd-76562a7c8c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EUAnAqC-s4uz"
   },
   "outputs": [],
   "source": [
    "all = pd.read_parquet('./data/train_all.parquet')\n",
    "orders = pd.read_parquet('./data/train_orders.parquet')\n",
    "ancestors = pd.read_parquet('./data/train_ancestors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to subset the data in order to test training or validation logic.\n",
    "\n",
    "# N_SAMPLES = 100\n",
    "# sample_ids = random.sample(list(all['id'].unique()), N_SAMPLES)\n",
    "# all = all.set_index('id').loc[sample_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PZrOUsXb88AV"
   },
   "outputs": [],
   "source": [
    "# Orders dataframe currently contains cell orders as a string, i.e \"a b c\"\n",
    "# We want to convert that into a list of strings: [\"a\", \"b\", \"c\"]\n",
    "orders['cell_order'] = orders['cell_order'].str.split(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hkBSYrzR06Qk"
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def links_to_word(text):\n",
    "    return re.sub(\"https?:\\/\\/[^\\s]+\", \" link \", text)\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    \"\"\"Remove reserved markdown special characters.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[\\.\\*\\+\\-\\_\\>\\<\\~\\(\\)\\[\\]]\", \" \", text)\n",
    "\n",
    "def no_html_tags(text):\n",
    "    return re.sub(\"<.*?>\", \" \", text)\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    try:\n",
    "        text = text[0] + re.sub(r\"(?<!\\n)[\\*\\+\\-\\>]\", \" \", text[1:])\n",
    "        text = re.sub(r\"\\(\\)\\[\\]\\{\\}\\<\\>\\~\\|\\`\\.\", \" \", text)\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "def code_preprocess(code):\n",
    "    code = links_to_word(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def markdown_preprocess(code: str):\n",
    "    \"\"\"\n",
    "    1. Replace new lines with unused token.\n",
    "    2. Remove HTML Tags and special markdown symbols.\n",
    "    3. Clear html tags first, then markdown...\n",
    "    \"\"\"\n",
    "    code = code.replace(\"\\n\", \"[unused1]\")\n",
    "    code = links_to_word(code)\n",
    "    code = no_html_tags(code)\n",
    "    code = no_markdown_special(code)\n",
    "    code = no_multi_spaces(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def preprocessor(text: str, cell_type: str):\n",
    "    return dict(code=code_preprocess, markdown=markdown_preprocess)[cell_type](text)\n",
    "\n",
    "def sample_cells(cells, n):\n",
    "    \"\"\"\n",
    "    Picking 20 cells for global context.\n",
    "    \"\"\"\n",
    "    cells = [code_preprocess(cell) for cell in cells]\n",
    "    if n >= len(cells):\n",
    "        return [cell[:200] for cell in cells]\n",
    "    else:\n",
    "        results = []\n",
    "        step = len(cells) / n\n",
    "        idx = 0\n",
    "        while int(np.round(idx)) < len(cells):\n",
    "            results.append(cells[int(np.round(idx))])\n",
    "            idx += step\n",
    "        assert cells[0] in results\n",
    "        if cells[-1] not in results:\n",
    "            results[-1] = cells[-1]\n",
    "        return results\n",
    "\n",
    "def get_features(df):\n",
    "    features = dict()\n",
    "\n",
    "    # Group by notebook and loop through unique notebooks.\n",
    "    for idx, sub_df in tqdm(df.groupby(\"id\")):\n",
    "        features[idx] = dict()\n",
    "\n",
    "        # Get count of markdown cells in current notebook.\n",
    "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
    "\n",
    "        # Get count of code cells in current notebook.\n",
    "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
    "        total_code = code_sub_df.shape[0]\n",
    "\n",
    "        # Sample 20 code cells.\n",
    "        # codes = sample_cells(code_sub_df.source.values, 20)\n",
    "        codes = code_sub_df.source.values\n",
    "        features[idx][\"total_code\"] = total_code\n",
    "        features[idx][\"total_md\"] = total_md\n",
    "        features[idx][\"codes\"] = codes\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Pp_oKQuTsHi_"
   },
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, code_model: str, markdown_model: str):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.code_model = AutoModel.from_pretrained(code_model)\n",
    "        self.markdown_model = AutoModel.from_pretrained(markdown_model)\n",
    "\n",
    "        # Bert embeddings are 768-d + 1 for code cell percentage.\n",
    "        self.top = nn.Linear(1536, 1)\n",
    "\n",
    "    def forward(self, code_ids, code_mask, markdown_ids, markdown_mask):\n",
    "        # Embeddings\n",
    "        code_embeddings = self.code_model(code_ids, code_mask)[0]\n",
    "        markdown_embeddings = self.markdown_model(markdown_ids, markdown_mask)[0]\n",
    "\n",
    "        # Concatenate code embeddings with markdown.\n",
    "        x = torch.cat((code_embeddings[:, 0, :], markdown_embeddings[:, 0, :]), 1)\n",
    "\n",
    "        return self.top(x)\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "    \"\"\"Encapsulates Markdown dataset into a single object.\n",
    "\n",
    "    :param markdown_rows: Pandas dataframe containing markdown content.\n",
    "    :param features: Extra features (number code cells, \n",
    "    :param md_max_len: Maximum length of markdown tokenized embedding.\n",
    "    :param total_max_len: Maximum Length of the tokenized input to bert.\n",
    "    :param model_name: Name of pretrained bert base model.\n",
    "\n",
    "    :attr code_model_name: Code bert model name.\n",
    "    :attr markdown_model_name: Bert model name.\n",
    "    :\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        markdown_rows: pd.DataFrame,\n",
    "        features: dict,\n",
    "        total_max_len: int,\n",
    "        md_max_len: int,\n",
    "        code_model_name: str = 'microsoft/codebert-base',\n",
    "        markdown_model_name: str = 'bert-base-uncased'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.markdown_rows = markdown_rows.reset_index(drop=True)\n",
    "        self.features = features\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len\n",
    "        self.markdown_model_name = markdown_model_name\n",
    "        self.code_model_name = code_model_name\n",
    "        self.code_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.code_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.markdown_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.markdown_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.markdown_rows.iloc[index]\n",
    "\n",
    "        # Encode markdown into embedding.\n",
    "        markdown_inputs = self.markdown_tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Encode code into embedding.\n",
    "        # Batch encode does not like empty lists!\n",
    "        code_cells = self.features[row.id][\"codes\"]\n",
    "        code_inputs = self.code_tokenizer.batch_encode_plus(\n",
    "            [str(cell) for cell in code_cells] if len(code_cells) > 0 else [''],\n",
    "            add_special_tokens=True,\n",
    "            max_length=23,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Get markdown embedding tokens.\n",
    "        markdown_ids = markdown_inputs['input_ids']\n",
    "        markdown_ids = markdown_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(markdown_ids) < self.total_max_len:\n",
    "            markdown_ids = markdown_ids + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_ids))\n",
    "\n",
    "        markdown_ids = torch.LongTensor(markdown_ids)\n",
    "\n",
    "        # Get markdown embedding tokens.\n",
    "        code_ids = list(np.array(code_inputs['input_ids']).flatten())\n",
    "        code_ids = code_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(code_ids) < self.total_max_len:\n",
    "            code_ids = code_ids + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_ids))\n",
    "\n",
    "        code_ids = torch.LongTensor(code_ids)\n",
    "\n",
    "        # Markdown masks\n",
    "        markdown_mask = markdown_inputs['attention_mask']\n",
    "        markdown_mask = markdown_mask[:self.total_max_len]\n",
    "\n",
    "        if len(markdown_mask) != self.total_max_len:\n",
    "            markdown_mask = markdown_mask + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_mask))\n",
    "        markdown_mask = torch.LongTensor(markdown_mask)\n",
    "\n",
    "        # Do the same for the code attention mask.\n",
    "        code_mask = markdown_inputs['attention_mask']\n",
    "        code_mask = code_mask[:self.total_max_len]\n",
    "\n",
    "        if len(code_mask) != self.total_max_len:\n",
    "            code_mask = code_mask + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_mask))\n",
    "        code_mask = torch.LongTensor(code_mask)\n",
    "\n",
    "        # Tokens should be equal to the maximum length.\n",
    "        assert len(markdown_ids) == self.total_max_len\n",
    "        assert len(code_ids) == self.total_max_len\n",
    "\n",
    "        # Tokens, attention mask, markdown percentage feature, and label.\n",
    "        return code_ids, code_mask, markdown_ids, markdown_mask, torch.FloatTensor([row.pct_rank])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.markdown_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VCnB0hN_E9L_"
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\"\"\"\n",
    "Pulled evaluation metric directly from Kaggle.\n",
    "\"\"\"\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMS1TkJ7_8sW",
    "outputId": "304d1930-dd30-47e6-8b5b-b070da76060b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 1102.68it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 1106.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 890.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Create label.\n",
    "all['pct_rank'] = all['order'] / all.groupby(\"id\")[\"cell\"].transform(\"count\")\n",
    "\n",
    "VALID_RATIO = 0.3\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "train_splitter = GroupShuffleSplit(n_splits=1, test_size=VALID_RATIO+TEST_RATIO, random_state=0)\n",
    "val_splitter = GroupShuffleSplit(n_splits=1, test_size=TEST_RATIO, random_state=0)\n",
    "\n",
    "# Split into train, (val + test) - 60% - 40%.\n",
    "train_ind, val_ind = next(train_splitter.split(all, groups=all[\"ancestor_id\"]))\n",
    "\n",
    "train_df = all.loc[train_ind].reset_index(drop=True)\n",
    "train_features = get_features(train_df)\n",
    "\n",
    "val_test_df = all.loc[val_ind].reset_index(drop=True)\n",
    "\n",
    "# Split val into val, test - 90% - 10%.\n",
    "val_ind, test_ind = next(val_splitter.split(val_test_df, groups=val_test_df[\"ancestor_id\"]))\n",
    "\n",
    "val_df = val_test_df.loc[val_ind].reset_index(drop=True)\n",
    "val_features = get_features(val_df)\n",
    "\n",
    "test_df = val_test_df.loc[test_ind].reset_index(drop=True)\n",
    "test_features = get_features(test_df)\n",
    "\n",
    "# Final sizes:\n",
    "# Train - 60%\n",
    "# Validation - 30%\n",
    "# Test - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8h_OGe-0wi7l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "1675\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape[0])\n",
    "print(val_df.shape[0])\n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r5Xr62u52pTE"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014246463775634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 28,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea712e1c42f48bfb6837ba600b2e567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013766050338745117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779de71da4054c5bbdafc6f07a6db31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014180898666381836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3edddc704466c83b4bdd77f4b3871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017153263092041016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 466062,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ae484090eb4cb491a7a70e21ae8386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markdown_train = train_df[train_df['cell_type'] == 'markdown']\n",
    "markdown_val = val_df[val_df['cell_type'] == 'markdown']\n",
    "markdown_test = test_df[test_df['cell_type'] == 'markdown']\n",
    "\n",
    "train_ds = MarkdownDataset(\n",
    "    markdown_train,\n",
    "    features = train_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "val_ds = MarkdownDataset(\n",
    "    markdown_val,\n",
    "    features = val_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "test_ds = MarkdownDataset(\n",
    "    markdown_test,\n",
    "    features = test_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QKsXWHNr_Vuq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    epochs=1,\n",
    "    lr=3e-5,\n",
    "    patience = 5,\n",
    "    use_wandb=False\n",
    "):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    early_stop_count = 0\n",
    "    best_loss = 1_000_000\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "    # Creating optimizer and lr schedulers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_train_optimization_steps = int(epochs * len(train_loader) / 4)\n",
    "\n",
    "    # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=lr,\n",
    "        correct_bias=False\n",
    "    )  \n",
    "\n",
    "    # PyTorch scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "        num_training_steps=num_train_optimization_steps\n",
    "    )  \n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        # Train\n",
    "        for idx, data in enumerate(tbar):\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "            \n",
    "            # Compute loss\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "            # Backprop\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update optimizer and scheduler.\n",
    "            if idx % 4 == 0 or idx == len(tbar) - 1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "            # Compute mean loss.\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            if idx % 25_000 == 0 and avg_loss < best_loss:\n",
    "                # Track best performance, and save the model's state\n",
    "                best_loss = avg_loss\n",
    "                model_path = 'models/{}_{}_{}_{}'.format(model_name, timestamp, e, best_loss)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "      \n",
    "            if idx % 1000 == 0:\n",
    "                if use_wandb:\n",
    "                    wandb.log({\n",
    "                        'avg_loss': avg_loss,\n",
    "                        'best_loss': best_loss\n",
    "                    })\n",
    "                \n",
    "            # Early stopping\n",
    "            if avg_loss > best_loss:\n",
    "                early_stop_count += 1\n",
    "                \n",
    "                if early_stop_count > patience:\n",
    "                    model_path = 'models/{}_{}_{}_{}'.format(model_name, timestamp, e, best_loss)\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                early_stop_count = 0\n",
    "                       \n",
    "            # Update progress bar.\n",
    "            tbar.set_description(f\"Epoch {e + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "    _, y_pred = np.concatenate(labels), np.concatenate(preds)\n",
    "\n",
    "    # Create a placeholder prediction.\n",
    "    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"order\"].rank(pct=True)\n",
    "    \n",
    "    # Replace pred column with predictions (only markdown cells since only markdown cells\n",
    "    # are randomized).\n",
    "    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n",
    "    \n",
    "    # Sort based on the predicted ranks, then obtain the order of cells as a list.\n",
    "    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell'].apply(list)\n",
    "    \n",
    "    # Get predictions in the same format as actuals.\n",
    "    prediction_cell_orders = y_dummy.to_frame()['cell']\n",
    "    \n",
    "    # Based on the notebook index, obtain the actual order from orders dataframe.\n",
    "    actual_cell_orders = orders.set_index('id').loc[y_dummy.index]['cell_order']\n",
    "    \n",
    "    # Compute metric.\n",
    "    kendall_tau_score = kendall_tau(actual_cell_orders, prediction_cell_orders)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'kendall_tau': kendall_tau_score\n",
    "        })\n",
    "        \n",
    "    print(\"Preds score\", kendall_tau_score)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fAYXLQmXE6os"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2849 lr: [0.0, 0.0]: 100%|██████████| 1108/1108 [02:06<00:00,  8.73it/s]                                      \n",
      "100%|██████████| 589/589 [00:17<00:00, 33.12it/s]\n",
      "Preds score 0.6551358101779582\n"
     ]
    }
   ],
   "source": [
    "WANDB = True\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 1,\n",
    "    'model_name': 'code-markdown-bert',\n",
    "    'patience': 100\n",
    "}\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"w266-project\",\n",
    "        entity=\"sotoodaa\",\n",
    "        name='markdown-model-code-markdown-bert',\n",
    "        config=MODEL_CONFIG\n",
    "    )\n",
    "    \n",
    "model = MarkdownModel('microsoft/codebert-base', 'bert-base-uncased')\n",
    "model = model.cuda()\n",
    "\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq=1000)\n",
    "    \n",
    "model, y_pred = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    MODEL_CONFIG['model_name'],\n",
    "    epochs=MODEL_CONFIG['epochs'],\n",
    "    lr=MODEL_CONFIG['lr'],\n",
    "    patience=MODEL_CONFIG['patience'],\n",
    "    use_wandb=WANDB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "if model:\n",
    "    model.cpu()\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
