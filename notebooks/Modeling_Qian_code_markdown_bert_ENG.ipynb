{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgvmxkHtHVtd",
    "outputId": "f1c06018-60de-444f-e310-d2c8f20b8b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.9.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.10.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Building jupyterlab assets (build:prod:minimize)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Kmx0M90i49_",
    "outputId": "06bf19d9-56af-4e0d-c7fd-76562a7c8c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EUAnAqC-s4uz"
   },
   "outputs": [],
   "source": [
    "all = pd.read_parquet('./data/english_notebook.parquet')\n",
    "orders = pd.read_parquet('./data/train_orders.parquet')\n",
    "ancestors = pd.read_parquet('./data/train_ancestors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to subset the data in order to test training or validation logic.\n",
    "\n",
    "# N_SAMPLES = 100\n",
    "# sample_ids = random.sample(list(all['id'].unique()), N_SAMPLES)\n",
    "# all = all.set_index('id').loc[sample_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "PZrOUsXb88AV"
   },
   "outputs": [],
   "source": [
    "# Orders dataframe currently contains cell orders as a string, i.e \"a b c\"\n",
    "# We want to convert that into a list of strings: [\"a\", \"b\", \"c\"]\n",
    "orders['cell_order'] = orders['cell_order'].str.split(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>[1862f0a6, 448eb224, 2a9e43d6, 7e2f170a, 038b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>[2e94bd7a, 3e99dee9, b5e286ea, da4f7550, c4172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001bdd4021779</td>\n",
       "      <td>[3fdc37be, 073782ca, 8ea7263c, 80543cd8, 38310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001daf4c2c76d</td>\n",
       "      <td>[97266564, a898e555, 86605076, 76cc2642, ef279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002115f48f982</td>\n",
       "      <td>[9ec225f0, 18281c6c, e3b6b115, 4a044c54, 365fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00035108e64677</td>\n",
       "      <td>[3496fbfe, 2fa1f27b, 719854c4, f3c2de19, d75fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00038c2941faa0</td>\n",
       "      <td>[3e551fb7, 45049ad8, 8bb41691, 123b4f4c, 0b92c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00039fc77a9dd1</td>\n",
       "      <td>[6d6a3714, 93a464db, 323458c8, 37966b23, 21d57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00058dc97eae4b</td>\n",
       "      <td>[4e0e17d0, 9edb9716, caa86438, 0c219582, f602e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000597ac4c6700</td>\n",
       "      <td>[ba54a747, ede4241f, fb42ece2, 36b989a3, 2fa55...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                         cell_order\n",
       "0  00001756c60be8  [1862f0a6, 448eb224, 2a9e43d6, 7e2f170a, 038b7...\n",
       "1  00015c83e2717b  [2e94bd7a, 3e99dee9, b5e286ea, da4f7550, c4172...\n",
       "2  0001bdd4021779  [3fdc37be, 073782ca, 8ea7263c, 80543cd8, 38310...\n",
       "3  0001daf4c2c76d  [97266564, a898e555, 86605076, 76cc2642, ef279...\n",
       "4  0002115f48f982  [9ec225f0, 18281c6c, e3b6b115, 4a044c54, 365fe...\n",
       "5  00035108e64677  [3496fbfe, 2fa1f27b, 719854c4, f3c2de19, d75fe...\n",
       "6  00038c2941faa0  [3e551fb7, 45049ad8, 8bb41691, 123b4f4c, 0b92c...\n",
       "7  00039fc77a9dd1  [6d6a3714, 93a464db, 323458c8, 37966b23, 21d57...\n",
       "8  00058dc97eae4b  [4e0e17d0, 9edb9716, caa86438, 0c219582, f602e...\n",
       "9  000597ac4c6700  [ba54a747, ede4241f, fb42ece2, 36b989a3, 2fa55..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>order</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>language</th>\n",
       "      <th>score</th>\n",
       "      <th>iso_language_name_x</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>2e94bd7a</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## Note : This Kernel is a Fork from the amazi...</td>\n",
       "      <td>0</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>3e99dee9</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## About The Competition : Detecting Steel Def...</td>\n",
       "      <td>1</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>b5e286ea</td>\n",
       "      <td>markdown</td>\n",
       "      <td>Reference :http://faculty.neu.edu.cn/yunhyan/...</td>\n",
       "      <td>2</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>da4f7550</td>\n",
       "      <td>markdown</td>\n",
       "      <td>## import modules and define models</td>\n",
       "      <td>3</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>c417225b</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebra\\nimport pa...</td>\n",
       "      <td>4</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.043011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>51e3cd89</td>\n",
       "      <td>code</td>\n",
       "      <td>input_dir = \"../input/\"</td>\n",
       "      <td>5</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.053763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>2600b4eb</td>\n",
       "      <td>code</td>\n",
       "      <td>train_df = pd.read_csv(\"../input/train.csv\")\\n...</td>\n",
       "      <td>6</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>75b65993</td>\n",
       "      <td>code</td>\n",
       "      <td>class_dict = defaultdict(int)\\n\\nkind_class_di...</td>\n",
       "      <td>7</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.075269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>cf195f8b</td>\n",
       "      <td>code</td>\n",
       "      <td>print(\"the number of images with no defects: {...</td>\n",
       "      <td>8</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.086022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00015c83e2717b</td>\n",
       "      <td>25699d02</td>\n",
       "      <td>code</td>\n",
       "      <td>fig, ax = plt.subplots()\\nsns.barplot(x=list(c...</td>\n",
       "      <td>9</td>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>English</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      cell cell_type  \\\n",
       "0  00015c83e2717b  2e94bd7a  markdown   \n",
       "1  00015c83e2717b  3e99dee9  markdown   \n",
       "2  00015c83e2717b  b5e286ea  markdown   \n",
       "3  00015c83e2717b  da4f7550  markdown   \n",
       "4  00015c83e2717b  c417225b      code   \n",
       "5  00015c83e2717b  51e3cd89      code   \n",
       "6  00015c83e2717b  2600b4eb      code   \n",
       "7  00015c83e2717b  75b65993      code   \n",
       "8  00015c83e2717b  cf195f8b      code   \n",
       "9  00015c83e2717b  25699d02      code   \n",
       "\n",
       "                                              source  order ancestor_id  \\\n",
       "0  ## Note : This Kernel is a Fork from the amazi...      0    aa2da37e   \n",
       "1  ## About The Competition : Detecting Steel Def...      1    aa2da37e   \n",
       "2   Reference :http://faculty.neu.edu.cn/yunhyan/...      2    aa2da37e   \n",
       "3                ## import modules and define models      3    aa2da37e   \n",
       "4  import numpy as np # linear algebra\\nimport pa...      4    aa2da37e   \n",
       "5                            input_dir = \"../input/\"      5    aa2da37e   \n",
       "6  train_df = pd.read_csv(\"../input/train.csv\")\\n...      6    aa2da37e   \n",
       "7  class_dict = defaultdict(int)\\n\\nkind_class_di...      7    aa2da37e   \n",
       "8  print(\"the number of images with no defects: {...      8    aa2da37e   \n",
       "9  fig, ax = plt.subplots()\\nsns.barplot(x=list(c...      9    aa2da37e   \n",
       "\n",
       "        parent_id language     score iso_language_name_x  pct_rank  \n",
       "0  317b65d12af9df       en  0.999998             English  0.000000  \n",
       "1  317b65d12af9df       en  0.999998             English  0.010753  \n",
       "2  317b65d12af9df       en  0.999998             English  0.021505  \n",
       "3  317b65d12af9df       en  0.999998             English  0.032258  \n",
       "4  317b65d12af9df       en  0.999998             English  0.043011  \n",
       "5  317b65d12af9df       en  0.999998             English  0.053763  \n",
       "6  317b65d12af9df       en  0.999998             English  0.064516  \n",
       "7  317b65d12af9df       en  0.999998             English  0.075269  \n",
       "8  317b65d12af9df       en  0.999998             English  0.086022  \n",
       "9  317b65d12af9df       en  0.999998             English  0.096774  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "hkBSYrzR06Qk"
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def links_to_word(text):\n",
    "    return re.sub(\"https?:\\/\\/[^\\s]+\", \" link \", text)\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    \"\"\"Remove reserved markdown special characters.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[\\.\\*\\+\\-\\_\\>\\<\\~\\(\\)\\[\\]]\", \" \", text)\n",
    "\n",
    "def no_html_tags(text):\n",
    "    return re.sub(\"<.*?>\", \" \", text)\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    try:\n",
    "        text = text[0] + re.sub(r\"(?<!\\n)[\\*\\+\\-\\>]\", \" \", text[1:])\n",
    "        text = re.sub(r\"\\(\\)\\[\\]\\{\\}\\<\\>\\~\\|\\`\\.\", \" \", text)\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "def code_preprocess(code):\n",
    "    code = links_to_word(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def markdown_preprocess(code: str):\n",
    "    \"\"\"\n",
    "    1. Replace new lines with unused token.\n",
    "    2. Remove HTML Tags and special markdown symbols.\n",
    "    3. Clear html tags first, then markdown...\n",
    "    \"\"\"\n",
    "    code = code.replace(\"\\n\", \"[unused1]\")\n",
    "    code = links_to_word(code)\n",
    "    code = no_html_tags(code)\n",
    "    code = no_markdown_special(code)\n",
    "    code = no_multi_spaces(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def preprocessor(text: str, cell_type: str):\n",
    "    return dict(code=code_preprocess, markdown=markdown_preprocess)[cell_type](text)\n",
    "\n",
    "def sample_cells(cells, n):\n",
    "    \"\"\"\n",
    "    Picking 20 cells for global context.\n",
    "    \"\"\"\n",
    "    cells = [code_preprocess(cell) for cell in cells]\n",
    "    if n >= len(cells):\n",
    "        return [cell[:200] for cell in cells]\n",
    "    else:\n",
    "        results = []\n",
    "        step = len(cells) / n\n",
    "        idx = 0\n",
    "        while int(np.round(idx)) < len(cells):\n",
    "            results.append(cells[int(np.round(idx))])\n",
    "            idx += step\n",
    "        assert cells[0] in results\n",
    "        if cells[-1] not in results:\n",
    "            results[-1] = cells[-1]\n",
    "        return results\n",
    "\n",
    "def get_features(df):\n",
    "    features = dict()\n",
    "\n",
    "    # Group by notebook and loop through unique notebooks.\n",
    "    for idx, sub_df in tqdm(df.groupby(\"id\")):\n",
    "        features[idx] = dict()\n",
    "\n",
    "        # Get count of markdown cells in current notebook.\n",
    "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
    "\n",
    "        # Get count of code cells in current notebook.\n",
    "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
    "        total_code = code_sub_df.shape[0]\n",
    "\n",
    "        # Sample 20 code cells.\n",
    "        # codes = sample_cells(code_sub_df.source.values, 20)\n",
    "        codes = code_sub_df.source.values\n",
    "        features[idx][\"total_code\"] = total_code\n",
    "        features[idx][\"total_md\"] = total_md\n",
    "        features[idx][\"codes\"] = codes\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_max_len = 512\n",
    "model_path = \"./graphcodebert-base-model\"\n",
    "#tokenizer_path = \"./graphcodebert-base-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Pp_oKQuTsHi_"
   },
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, model_path: str, markdown_model: str):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.code_model = AutoModel.from_pretrained(model_path)\n",
    "        self.markdown_model = AutoModel.from_pretrained(markdown_model)\n",
    "        #self.model = AutoModel.from_pretrained(model_path)\n",
    "        #self.top = nn.Linear(769, 1)\n",
    "#         self.code_model = AutoModel.from_pretrained(code_model)\n",
    "#         self.markdown_model = AutoModel.from_pretrained(markdown_model)\n",
    "\n",
    "        # Bert embeddings are 768-d + 1 for code cell percentage.\n",
    "        self.top = nn.Linear(1536, 1)\n",
    "\n",
    "    def forward(self, code_ids, code_mask, markdown_ids, markdown_mask):\n",
    "        # Embeddings\n",
    "        code_embeddings = self.code_model(code_ids, code_mask)[0]\n",
    "        markdown_embeddings = self.markdown_model(markdown_ids, markdown_mask)[0]\n",
    "\n",
    "        # Concatenate code embeddings with markdown.\n",
    "        x = torch.cat((code_embeddings[:, 0, :], markdown_embeddings[:, 0, :]), 1)\n",
    "\n",
    "        return self.top(x)\n",
    "#     def forward(self, ids, mask, features):\n",
    "#         # Embeddings\n",
    "#         x = self.model(ids, mask)[0]\n",
    "        \n",
    "#         x = self.top(torch.cat((x[:, 0, :], features),1))\n",
    "#         return x\n",
    "\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "    \"\"\"Encapsulates Markdown dataset into a single object.\n",
    "\n",
    "    :param markdown_rows: Pandas dataframe containing markdown content.\n",
    "    :param features: Extra features (number code cells, \n",
    "    :param md_max_len: Maximum length of markdown tokenized embedding.\n",
    "    :param total_max_len: Maximum Length of the tokenized input to bert.\n",
    "    :param model_name: Name of pretrained bert base model.\n",
    "\n",
    "    :attr code_model_name: GraphCode bert model name.\n",
    "    :attr markdown_model_name: Bert model name.\n",
    "    :\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        markdown_rows: pd.DataFrame,\n",
    "        features: dict,\n",
    "        total_max_len: int,\n",
    "        md_max_len: int,\n",
    "        code_model_name: str = 'microsoft/graphcodebert-base',\n",
    "        markdown_model_name: str = 'bert-base-uncased'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.markdown_rows = markdown_rows.reset_index(drop=True)\n",
    "        self.features = features\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len\n",
    "        self.markdown_model_name = markdown_model_name\n",
    "        self.code_model_name = code_model_name\n",
    "        self.code_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.code_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.markdown_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.markdown_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "                            )\n",
    "         \n",
    "#         self.markdown_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             self.markdown_model_name,\n",
    "#             do_lower_case=True,\n",
    "#             use_fast=True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.markdown_rows.iloc[index]\n",
    "        \n",
    "        # Encode markdown into embedding.\n",
    "        markdown_inputs = self.markdown_tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Encode code into embedding.\n",
    "        # Batch encode does not like empty lists!\n",
    "        code_cells = self.features[row.id][\"codes\"]\n",
    "        code_inputs = self.code_tokenizer.batch_encode_plus(\n",
    "            [str(cell) for cell in code_cells] if len(code_cells) > 0 else [''],\n",
    "            add_special_tokens=True,\n",
    "            max_length=23,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # Get markdown embedding tokens.\n",
    "        markdown_ids = markdown_inputs['input_ids']\n",
    "        markdown_ids = markdown_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(markdown_ids) < self.total_max_len:\n",
    "            markdown_ids = markdown_ids + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_ids))\n",
    "\n",
    "        markdown_ids = torch.LongTensor(markdown_ids)\n",
    "\n",
    "        # Get code embedding tokens.\n",
    "        code_ids = list(np.array(code_inputs['input_ids']).flatten())\n",
    "        code_ids = code_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(code_ids) < self.total_max_len:\n",
    "            code_ids = code_ids + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_ids))\n",
    "\n",
    "        code_ids = torch.LongTensor(code_ids)\n",
    "        \n",
    "        # Markdown masks\n",
    "        markdown_mask = markdown_inputs['attention_mask']\n",
    "        markdown_mask = markdown_mask[:self.total_max_len]\n",
    "\n",
    "        if len(markdown_mask) != self.total_max_len:\n",
    "            markdown_mask = markdown_mask + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_mask))\n",
    "        markdown_mask = torch.LongTensor(markdown_mask)\n",
    "\n",
    "        # Do the same for the code attention mask.\n",
    "        code_mask = markdown_inputs['attention_mask']\n",
    "        code_mask = code_mask[:self.total_max_len]\n",
    "\n",
    "        if len(code_mask) != self.total_max_len:\n",
    "            code_mask = code_mask + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_mask))\n",
    "        code_mask = torch.LongTensor(code_mask)\n",
    "\n",
    "        # Tokens should be equal to the maximum length.\n",
    "        assert len(markdown_ids) == self.total_max_len\n",
    "        assert len(code_ids) == self.total_max_len\n",
    "\n",
    "        # Tokens, attention mask, markdown percentage feature, and label.\n",
    "        return code_ids, code_mask, markdown_ids, markdown_mask, torch.FloatTensor([row.pct_rank])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.markdown_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "VCnB0hN_E9L_"
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\"\"\"\n",
    "Pulled evaluation metric directly from Kaggle.\n",
    "\"\"\"\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMS1TkJ7_8sW",
    "outputId": "304d1930-dd30-47e6-8b5b-b070da76060b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75517/75517 [01:07<00:00, 1123.27it/s]\n",
      "100%|██████████| 45135/45135 [00:40<00:00, 1125.16it/s]\n",
      "100%|██████████| 5061/5061 [00:04<00:00, 1119.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Create label.\n",
    "all['pct_rank'] = all['order'] / all.groupby(\"id\")[\"cell\"].transform(\"count\")\n",
    "\n",
    "VALID_RATIO = 0.3\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "train_splitter = GroupShuffleSplit(n_splits=1, test_size=VALID_RATIO+TEST_RATIO, random_state=0)\n",
    "val_splitter = GroupShuffleSplit(n_splits=1, test_size=TEST_RATIO, random_state=0)\n",
    "\n",
    "# Split into train, (val + test) - 60% - 40%.\n",
    "train_ind, val_ind = next(train_splitter.split(all, groups=all[\"ancestor_id\"]))\n",
    "\n",
    "train_df = all.loc[train_ind].reset_index(drop=True)\n",
    "train_features = get_features(train_df)\n",
    "\n",
    "val_test_df = all.loc[val_ind].reset_index(drop=True)\n",
    "\n",
    "# Split val into val, test - 90% - 10%.\n",
    "val_ind, test_ind = next(val_splitter.split(val_test_df, groups=val_test_df[\"ancestor_id\"]))\n",
    "\n",
    "val_df = val_test_df.loc[val_ind].reset_index(drop=True)\n",
    "val_features = get_features(val_df)\n",
    "\n",
    "test_df = val_test_df.loc[test_ind].reset_index(drop=True)\n",
    "test_features = get_features(test_df)\n",
    "\n",
    "# Final sizes:\n",
    "# Train - 60%\n",
    "# Validation - 30%\n",
    "# Test - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8h_OGe-0wi7l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482418\n",
      "2071964\n",
      "231228\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape[0])\n",
    "print(val_df.shape[0])\n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "r5Xr62u52pTE"
   },
   "outputs": [],
   "source": [
    "markdown_train = train_df[train_df['cell_type'] == 'markdown']\n",
    "markdown_val = val_df[val_df['cell_type'] == 'markdown']\n",
    "markdown_test = test_df[test_df['cell_type'] == 'markdown']\n",
    "\n",
    "train_ds = MarkdownDataset(\n",
    "    markdown_train,\n",
    "    features = train_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "val_ds = MarkdownDataset(\n",
    "    markdown_val,\n",
    "    features = val_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "test_ds = MarkdownDataset(\n",
    "    markdown_test,\n",
    "    features = test_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QKsXWHNr_Vuq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "MODEL_NAME = 'graphcodebert'\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    epochs=1,\n",
    "    lr=3e-5,\n",
    "    patience = 5,\n",
    "    use_wandb=False\n",
    "):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    early_stop_count = 0\n",
    "    patience = 5\n",
    "    best_loss = 1_000_000\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "   \n",
    "    # Creating optimizer and lr schedulers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_train_optimization_steps = int(epochs * len(train_loader) / 4)\n",
    "\n",
    "    # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        #lr=3e-5,\n",
    "        lr=lr,\n",
    "        correct_bias=False\n",
    "    )  \n",
    "\n",
    "    # PyTorch scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "        num_training_steps=num_train_optimization_steps\n",
    "    )  \n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        # Train\n",
    "        for idx, data in enumerate(tbar):\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "            \n",
    "            # Compute loss\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "            # Backprop\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update optimizer and scheduler.\n",
    "            if idx % 4 == 0 or idx == len(tbar) - 1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "            # Compute mean loss.\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            if idx % 25_000 == 0 and avg_loss < best_loss:\n",
    "                # Track best performance, and save the model's state\n",
    "                best_loss = avg_loss\n",
    "                model_path = 'models/{}_{}_{}_{}'.format(MODEL_NAME, timestamp, e, best_loss)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            if idx % 1000 == 0 and avg_loss < best_loss:\n",
    "                wandb.log({\n",
    "                    'avg_loss': avg_loss,\n",
    "                    'best_loss': best_loss\n",
    "                })\n",
    "                early_stop_count = 0\n",
    "                \n",
    "            # Early stopping\n",
    "            if avg_loss > best_loss:\n",
    "                early_stop_count += 1\n",
    "                \n",
    "                if early_stop_count > patience:\n",
    "                    model_path = 'models/{}_{}_{}_{}'.format(MODEL_NAME, timestamp, e, best_loss)\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    break\n",
    "                       \n",
    "            # Update progress bar.\n",
    "            tbar.set_description(f\"Epoch {e + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "    _, y_pred = np.concatenate(labels), np.concatenate(preds)\n",
    "\n",
    "    # Create a placeholder prediction.\n",
    "    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"order\"].rank(pct=True)\n",
    "    \n",
    "    # Replace pred column with predictions (only markdown cells since only markdown cells\n",
    "    # are randomized).\n",
    "    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n",
    "    \n",
    "    # Sort based on the predicted ranks, then obtain the order of cells as a list.\n",
    "    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell'].apply(list)\n",
    "    \n",
    "    # Get predictions in the same format as actuals.\n",
    "    prediction_cell_orders = y_dummy.to_frame()['cell']\n",
    "    \n",
    "    # Based on the notebook index, obtain the actual order from orders dataframe.\n",
    "    actual_cell_orders = orders.set_index('id').loc[y_dummy.index]['cell_order']\n",
    "    \n",
    "    # Compute metric.\n",
    "    kendall_tau_score = kendall_tau(actual_cell_orders, prediction_cell_orders)\n",
    "    print(\"Preds score\", kendall_tau_score)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "fAYXLQmXE6os"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1cqxn029) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">markdown-model-graphcodebert</strong>: <a href=\"https://wandb.ai/sotoodaa/w266-project/runs/1cqxn029\" target=\"_blank\">https://wandb.ai/sotoodaa/w266-project/runs/1cqxn029</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_215210-1cqxn029/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1cqxn029). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2020ee6d4b40ffbf778ad9cd5d7e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666827980273714, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/w266-project/wandb/run-20221124_215840-2az6ll9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sotoodaa/w266-project/runs/2az6ll9v\" target=\"_blank\">code-markdown-model-graphcodebert-eng</a></strong> to <a href=\"https://wandb.ai/sotoodaa/w266-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sotoodaa/w266-project/runs/2az6ll9v?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6e01680be0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"w266-project\", entity=\"sotoodaa\", name='code-markdown-model-graphcodebert-eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2az6ll9v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">code-markdown-model-graphcodebert-eng</strong>: <a href=\"https://wandb.ai/sotoodaa/w266-project/runs/2az6ll9v\" target=\"_blank\">https://wandb.ai/sotoodaa/w266-project/runs/2az6ll9v</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221124_215840-2az6ll9v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2az6ll9v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f9b8ebd3cb48b4bb5b87fb261a5646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668231637837986, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/w266-project/wandb/run-20221124_215858-1aopy3se</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sotoodaa/w266-project/runs/1aopy3se\" target=\"_blank\">code-markdown-graphcodebert</a></strong> to <a href=\"https://wandb.ai/sotoodaa/w266-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1197178 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2858 lr: [2.672957025533422e-09, 2.672957025533422e-09]:   0%|          | 13/1197178 [00:03<87:03:00,  3.82it/s]  \n",
      "100%|██████████| 711043/711043 [6:12:25<00:00, 31.82it/s]    \n",
      "Preds score 0.5280339623659719\n"
     ]
    }
   ],
   "source": [
    "WANDB = True\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 1,\n",
    "    'model_name': 'code-markdown-graphcodebert',\n",
    "    'patience': 100\n",
    "}\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"w266-project\",\n",
    "        entity=\"sotoodaa\",\n",
    "        name='code-markdown-graphcodebert',\n",
    "        config=MODEL_CONFIG\n",
    "    )\n",
    "    \n",
    "model = MarkdownModel('microsoft/graphcodebert-base', 'bert-base-uncased')\n",
    "model = model.cuda()\n",
    "\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq=1000)\n",
    "    \n",
    "model, y_pred = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    MODEL_CONFIG['model_name'],\n",
    "    epochs=MODEL_CONFIG['epochs'],\n",
    "    lr=MODEL_CONFIG['lr'],\n",
    "    patience=MODEL_CONFIG['patience'],\n",
    "    use_wandb=WANDB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, log_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.0733 lr: [3.705773904557793e-09, 3.705773904557793e-09]:   0%|          | 6/1295276 [00:02<129:35:05,  2.78it/s]  \n",
      "100%|██████████| 784816/784816 [7:27:37<00:00, 29.22it/s]    \n",
      "Preds score 0.4658720608634791\n"
     ]
    }
   ],
   "source": [
    "model, y_pred = train(model, train_loader, val_loader, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: While tearing down the service manager. The following error has occured: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "if model:\n",
    "    model.cpu()\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
