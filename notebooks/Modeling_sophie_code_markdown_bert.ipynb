{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgvmxkHtHVtd",
    "outputId": "f1c06018-60de-444f-e310-d2c8f20b8b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.9.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.5)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.10.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.7.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Building jupyterlab assets (build:prod:minimize)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install wandb\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Kmx0M90i49_",
    "outputId": "06bf19d9-56af-4e0d-c7fd-76562a7c8c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EUAnAqC-s4uz"
   },
   "outputs": [],
   "source": [
    "all = pd.read_parquet('./data/english_notebook.parquet')\n",
    "orders = pd.read_parquet('./data/train_orders.parquet')\n",
    "ancestors = pd.read_parquet('./data/train_ancestors.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to subset the data in order to test training or validation logic.\n",
    "\n",
    "# N_SAMPLES = 100\n",
    "# sample_ids = random.sample(list(all['id'].unique()), N_SAMPLES)\n",
    "# all = all.set_index('id').loc[sample_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PZrOUsXb88AV"
   },
   "outputs": [],
   "source": [
    "# Orders dataframe currently contains cell orders as a string, i.e \"a b c\"\n",
    "# We want to convert that into a list of strings: [\"a\", \"b\", \"c\"]\n",
    "orders['cell_order'] = orders['cell_order'].str.split(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hkBSYrzR06Qk"
   },
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def links_to_word(text):\n",
    "    return re.sub(\"https?:\\/\\/[^\\s]+\", \" link \", text)\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    \"\"\"Remove reserved markdown special characters.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[\\.\\*\\+\\-\\_\\>\\<\\~\\(\\)\\[\\]]\", \" \", text)\n",
    "\n",
    "def no_html_tags(text):\n",
    "    return re.sub(\"<.*?>\", \" \", text)\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "def no_markdown_special(text):\n",
    "    try:\n",
    "        text = text[0] + re.sub(r\"(?<!\\n)[\\*\\+\\-\\>]\", \" \", text[1:])\n",
    "        text = re.sub(r\"\\(\\)\\[\\]\\{\\}\\<\\>\\~\\|\\`\\.\", \" \", text)\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "def code_preprocess(code):\n",
    "    code = links_to_word(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def markdown_preprocess(code: str):\n",
    "    \"\"\"\n",
    "    1. Replace new lines with unused token.\n",
    "    2. Remove HTML Tags and special markdown symbols.\n",
    "    3. Clear html tags first, then markdown...\n",
    "    \"\"\"\n",
    "    code = code.replace(\"\\n\", \"[unused1]\")\n",
    "    code = links_to_word(code)\n",
    "    code = no_html_tags(code)\n",
    "    code = no_markdown_special(code)\n",
    "    code = no_multi_spaces(code)\n",
    "    code = lemmatize(code)\n",
    "    return code\n",
    "\n",
    "def preprocessor(text: str, cell_type: str):\n",
    "    return dict(code=code_preprocess, markdown=markdown_preprocess)[cell_type](text)\n",
    "\n",
    "def sample_cells(cells, n):\n",
    "    \"\"\"\n",
    "    Picking 20 cells for global context.\n",
    "    \"\"\"\n",
    "    cells = [code_preprocess(cell) for cell in cells]\n",
    "    if n >= len(cells):\n",
    "        return [cell[:200] for cell in cells]\n",
    "    else:\n",
    "        results = []\n",
    "        step = len(cells) / n\n",
    "        idx = 0\n",
    "        while int(np.round(idx)) < len(cells):\n",
    "            results.append(cells[int(np.round(idx))])\n",
    "            idx += step\n",
    "        assert cells[0] in results\n",
    "        if cells[-1] not in results:\n",
    "            results[-1] = cells[-1]\n",
    "        return results\n",
    "\n",
    "def get_features(df):\n",
    "    features = dict()\n",
    "\n",
    "    # Group by notebook and loop through unique notebooks.\n",
    "    for idx, sub_df in tqdm(df.groupby(\"id\")):\n",
    "        features[idx] = dict()\n",
    "\n",
    "        # Get count of markdown cells in current notebook.\n",
    "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
    "\n",
    "        # Get count of code cells in current notebook.\n",
    "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
    "        total_code = code_sub_df.shape[0]\n",
    "\n",
    "        # Sample 20 code cells.\n",
    "        # codes = sample_cells(code_sub_df.source.values, 20)\n",
    "        codes = code_sub_df.source.values\n",
    "        features[idx][\"total_code\"] = total_code\n",
    "        features[idx][\"total_md\"] = total_md\n",
    "        features[idx][\"codes\"] = codes\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Pp_oKQuTsHi_"
   },
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, code_model: str, markdown_model: str):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.code_model = AutoModel.from_pretrained(code_model)\n",
    "        self.markdown_model = AutoModel.from_pretrained(markdown_model)\n",
    "\n",
    "        # Bert embeddings are 768-d + 1 for code cell percentage.       \n",
    "        self.top = nn.AvgPool1d(kernel_size = 2,\n",
    "                                   stride = None,\n",
    "                                   padding = 0,\n",
    "                                   ceil_mode = False,\n",
    "                                   count_include_pad = True)\n",
    "        self.out = nn.Linear(768//2, 1)\n",
    "\n",
    "    def forward(self, code_ids, code_mask, markdown_ids, markdown_mask):\n",
    "        # Embeddings\n",
    "        code_embeddings = self.code_model(code_ids, code_mask)[0]\n",
    "        markdown_embeddings = self.markdown_model(markdown_ids, markdown_mask)[0]\n",
    "        # Concatenate code embeddings with markdown.\n",
    "        x = torch.cat((code_embeddings[:, 0, :], markdown_embeddings[:, 0, :]), 1)\n",
    "        \n",
    "        return self.out(self.top(x))\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "    \"\"\"Encapsulates Markdown dataset into a single object.\n",
    "\n",
    "    :param markdown_rows: Pandas dataframe containing markdown content.\n",
    "    :param features: Extra features (number code cells, \n",
    "    :param md_max_len: Maximum length of markdown tokenized embedding.\n",
    "    :param total_max_len: Maximum Length of the tokenized input to bert.\n",
    "    :param model_name: Name of pretrained bert base model.\n",
    "\n",
    "    :attr code_model_name: Code bert model name.\n",
    "    :attr markdown_model_name: Bert model name.\n",
    "    :\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        markdown_rows: pd.DataFrame,\n",
    "        features: dict,\n",
    "        total_max_len: int,\n",
    "        md_max_len: int,\n",
    "        code_model_name: str = 'microsoft/codebert-base',\n",
    "        markdown_model_name: str = 'bert-base-uncased'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.markdown_rows = markdown_rows.reset_index(drop=True)\n",
    "        self.features = features\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len\n",
    "        self.markdown_model_name = markdown_model_name\n",
    "        self.code_model_name = code_model_name\n",
    "        self.code_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.code_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "        self.markdown_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.markdown_model_name,\n",
    "            do_lower_case=True,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.markdown_rows.iloc[index]\n",
    "\n",
    "        # Encode markdown into embedding.\n",
    "        markdown_inputs = self.markdown_tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Encode code into embedding.\n",
    "        # Batch encode does not like empty lists!\n",
    "        code_cells = self.features[row.id][\"codes\"]\n",
    "        code_inputs = self.code_tokenizer.batch_encode_plus(\n",
    "            [str(cell) for cell in code_cells] if len(code_cells) > 0 else [''],\n",
    "            add_special_tokens=True,\n",
    "            max_length=23,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Get markdown embedding tokens.\n",
    "        markdown_ids = markdown_inputs['input_ids']\n",
    "        markdown_ids = markdown_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(markdown_ids) < self.total_max_len:\n",
    "            markdown_ids = markdown_ids + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_ids))\n",
    "\n",
    "        markdown_ids = torch.LongTensor(markdown_ids)\n",
    "\n",
    "        # Get markdown embedding tokens.\n",
    "        code_ids = list(np.array(code_inputs['input_ids']).flatten())\n",
    "        code_ids = code_ids[:self.total_max_len]\n",
    "\n",
    "        # Apply padding if code + markdown tokens is less than max.\n",
    "        if len(code_ids) < self.total_max_len:\n",
    "            code_ids = code_ids + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_ids))\n",
    "\n",
    "        code_ids = torch.LongTensor(code_ids)\n",
    "\n",
    "        # Markdown masks\n",
    "        markdown_mask = markdown_inputs['attention_mask']\n",
    "        markdown_mask = markdown_mask[:self.total_max_len]\n",
    "\n",
    "        if len(markdown_mask) != self.total_max_len:\n",
    "            markdown_mask = markdown_mask + [self.markdown_tokenizer.pad_token_id, ] * (self.total_max_len - len(markdown_mask))\n",
    "        markdown_mask = torch.LongTensor(markdown_mask)\n",
    "\n",
    "        # Do the same for the code attention mask.\n",
    "        code_mask = markdown_inputs['attention_mask']\n",
    "        code_mask = code_mask[:self.total_max_len]\n",
    "\n",
    "        if len(code_mask) != self.total_max_len:\n",
    "            code_mask = code_mask + [self.code_tokenizer.pad_token_id, ] * (self.total_max_len - len(code_mask))\n",
    "        code_mask = torch.LongTensor(code_mask)\n",
    "\n",
    "        # Tokens should be equal to the maximum length.\n",
    "        assert len(markdown_ids) == self.total_max_len\n",
    "        assert len(code_ids) == self.total_max_len\n",
    "\n",
    "        # Tokens, attention mask, markdown percentage feature, and label.\n",
    "        return code_ids, code_mask, markdown_ids, markdown_mask, torch.FloatTensor([row.pct_rank])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.markdown_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VCnB0hN_E9L_"
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\"\"\"\n",
    "Pulled evaluation metric directly from Kaggle.\n",
    "\"\"\"\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMS1TkJ7_8sW",
    "outputId": "304d1930-dd30-47e6-8b5b-b070da76060b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75517/75517 [01:07<00:00, 1124.48it/s]\n",
      "100%|██████████| 45135/45135 [00:39<00:00, 1137.39it/s]\n",
      "100%|██████████| 5061/5061 [00:04<00:00, 1141.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Create label.\n",
    "all['pct_rank'] = all['order'] / all.groupby(\"id\")[\"cell\"].transform(\"count\")\n",
    "\n",
    "VALID_RATIO = 0.3\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "train_splitter = GroupShuffleSplit(n_splits=1, test_size=VALID_RATIO+TEST_RATIO, random_state=0)\n",
    "val_splitter = GroupShuffleSplit(n_splits=1, test_size=TEST_RATIO, random_state=0)\n",
    "\n",
    "# Split into train, (val + test) - 60% - 40%.\n",
    "train_ind, val_ind = next(train_splitter.split(all, groups=all[\"ancestor_id\"]))\n",
    "\n",
    "train_df = all.loc[train_ind].reset_index(drop=True)\n",
    "train_features = get_features(train_df)\n",
    "\n",
    "val_test_df = all.loc[val_ind].reset_index(drop=True)\n",
    "\n",
    "# Split val into val, test - 90% - 10%.\n",
    "val_ind, test_ind = next(val_splitter.split(val_test_df, groups=val_test_df[\"ancestor_id\"]))\n",
    "\n",
    "val_df = val_test_df.loc[val_ind].reset_index(drop=True)\n",
    "val_features = get_features(val_df)\n",
    "\n",
    "test_df = val_test_df.loc[test_ind].reset_index(drop=True)\n",
    "test_features = get_features(test_df)\n",
    "\n",
    "# Final sizes:\n",
    "# Train - 60%\n",
    "# Validation - 30%\n",
    "# Test - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8h_OGe-0wi7l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3482418\n",
      "2071964\n",
      "231228\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape[0])\n",
    "print(val_df.shape[0])\n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r5Xr62u52pTE"
   },
   "outputs": [],
   "source": [
    "markdown_train = train_df[train_df['cell_type'] == 'markdown']\n",
    "markdown_val = val_df[val_df['cell_type'] == 'markdown']\n",
    "markdown_test = test_df[test_df['cell_type'] == 'markdown']\n",
    "\n",
    "train_ds = MarkdownDataset(\n",
    "    markdown_train,\n",
    "    features = train_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "val_ds = MarkdownDataset(\n",
    "    markdown_val,\n",
    "    features = val_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "test_ds = MarkdownDataset(\n",
    "    markdown_test,\n",
    "    features = test_features,\n",
    "    total_max_len = 400,\n",
    "    md_max_len = 200\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QKsXWHNr_Vuq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name,\n",
    "    epochs=1,\n",
    "    lr=3e-5,\n",
    "    patience = 5,\n",
    "    use_wandb=False\n",
    "):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    early_stop_count = 0\n",
    "    best_loss = 1_000_000\n",
    "    best_vloss = 1_000_000\n",
    "\n",
    "    # Creating optimizer and lr schedulers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_train_optimization_steps = int(epochs * len(train_loader) / 4)\n",
    "\n",
    "    # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=lr,\n",
    "        correct_bias=False\n",
    "    )  \n",
    "\n",
    "    # PyTorch scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "        num_training_steps=num_train_optimization_steps\n",
    "    )  \n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        # Train\n",
    "        for idx, data in enumerate(tbar):\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "            \n",
    "            # Compute loss\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "            # Backprop\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update optimizer and scheduler.\n",
    "            if idx % 4 == 0 or idx == len(tbar) - 1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "            # Compute mean loss.\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            if idx % 25_000 == 0 and avg_loss < best_loss:\n",
    "                # Track best performance, and save the model's state\n",
    "                best_loss = avg_loss\n",
    "                model_path = 'models/{}_{}_{}_{}'.format(model_name, timestamp, e, best_loss)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "      \n",
    "            if idx % 1000 == 0:\n",
    "                if use_wandb:\n",
    "                    wandb.log({\n",
    "                        'avg_loss': avg_loss,\n",
    "                        'best_loss': best_loss\n",
    "                    })\n",
    "                \n",
    "            # Early stopping\n",
    "            if avg_loss > best_loss:\n",
    "                early_stop_count += 1\n",
    "                \n",
    "                if early_stop_count > patience:\n",
    "                    model_path = 'models/{}_{}_{}_{}'.format(model_name, timestamp, e, best_loss)\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                early_stop_count = 0\n",
    "                       \n",
    "            # Update progress bar.\n",
    "            tbar.set_description(f\"Epoch {e + 1} Loss: {avg_loss} lr: {scheduler.get_last_lr()}\")\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            code_ids, code_mask, markdown_ids, markdown_mask, target = [dp.cuda() for dp in data]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(code_ids, code_mask, markdown_ids, markdown_mask)\n",
    "\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "    _, y_pred = np.concatenate(labels), np.concatenate(preds)\n",
    "\n",
    "    # Create a placeholder prediction.\n",
    "    val_df[\"pred\"] = val_df.groupby([\"id\", \"cell_type\"])[\"order\"].rank(pct=True)\n",
    "    \n",
    "    # Replace pred column with predictions (only markdown cells since only markdown cells\n",
    "    # are randomized).\n",
    "    val_df.loc[val_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_pred\n",
    "    \n",
    "    # Sort based on the predicted ranks, then obtain the order of cells as a list.\n",
    "    y_dummy = val_df.sort_values(\"pred\").groupby('id')['cell'].apply(list)\n",
    "    \n",
    "    # Get predictions in the same format as actuals.\n",
    "    prediction_cell_orders = y_dummy.to_frame()['cell']\n",
    "    \n",
    "    # Based on the notebook index, obtain the actual order from orders dataframe.\n",
    "    actual_cell_orders = orders.set_index('id').loc[y_dummy.index]['cell_order']\n",
    "    \n",
    "    # Compute metric.\n",
    "    kendall_tau_score = kendall_tau(actual_cell_orders, prediction_cell_orders)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'kendall_tau': kendall_tau_score\n",
    "        })\n",
    "        \n",
    "    print(\"Preds score\", kendall_tau_score)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fAYXLQmXE6os"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msotoodaa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/w266-project/wandb/run-20221201_021629-k5zqq7uu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sotoodaa/w266-project/runs/k5zqq7uu\" target=\"_blank\">markdown-model-code-markdown-bert-avgpool</a></strong> to <a href=\"https://wandb.ai/sotoodaa/w266-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1197178 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x768 and 384x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m WANDB:\n\u001b[1;32m     22\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mwatch(model, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWANDB\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, model_name, epochs, lr, patience, use_wandb)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 67\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkdown_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkdown_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [7], line 22\u001b[0m, in \u001b[0;36mMarkdownModel.forward\u001b[0;34m(self, code_ids, code_mask, markdown_ids, markdown_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Concatenate code embeddings with markdown.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((code_embeddings[:, \u001b[38;5;241m0\u001b[39m, :], markdown_embeddings[:, \u001b[38;5;241m0\u001b[39m, :]), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x768 and 384x1)"
     ]
    }
   ],
   "source": [
    "WANDB = True\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 1,\n",
    "    'model_name': 'code-markdown-bert',\n",
    "    'patience': 100\n",
    "}\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"w266-project\",\n",
    "        entity=\"sotoodaa\",\n",
    "        name='markdown-model-code-markdown-bert-avgpool',\n",
    "        config=MODEL_CONFIG\n",
    "    )\n",
    "    \n",
    "model = MarkdownModel('microsoft/codebert-base', 'bert-base-uncased')\n",
    "model = model.cuda()\n",
    "\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq=1000)\n",
    "    \n",
    "model, y_pred = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    MODEL_CONFIG['model_name'],\n",
    "    epochs=MODEL_CONFIG['epochs'],\n",
    "    lr=MODEL_CONFIG['lr'],\n",
    "    patience=MODEL_CONFIG['patience'],\n",
    "    use_wandb=WANDB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "if model:\n",
    "    model.cpu()\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
